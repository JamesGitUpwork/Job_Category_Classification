{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for training and testing model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libary to save model / load models\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local db connection\n",
    "\n",
    "username=\"postgres\"\n",
    "password=\"test_passord_^1234\"\n",
    "db_host=\"localhost\"\n",
    "port='5432'\n",
    "database=\"winthrop_db\"\n",
    "\n",
    "engine = create_engine(f'postgresql+psycopg2://{username}:{password}@{db_host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonTargetData(target_category):\n",
    "    temp = '''\n",
    "    select job_id, concat(title, '. ', description ) as description from job_categorization_vw \n",
    "        where predicted_category != '{}'\n",
    "        '''\n",
    "    non_target_query = temp.format(target_category)\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        query = text(non_target_query)\n",
    "        rs = con.execute(query)\n",
    "\n",
    "        rows = rs.fetchall()\n",
    "\n",
    "        non_target_data = pd.DataFrame(rows,columns=['job_id','description'])\n",
    "\n",
    "    # Tag non-target data\n",
    "    non_target_data['tag'] = 0\n",
    "\n",
    "    return non_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetData(target_category):\n",
    "    temp = '''\n",
    "    select job_id, concat(title, '. ', description ) as description from corrected_categorization_tb \n",
    "        where tag = 1 and predicted_category = '{}'\n",
    "    '''\n",
    "    target_query = temp.format(target_category)\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        query = text(target_query)\n",
    "        rs = con.execute(query)\n",
    "\n",
    "        rows = rs.fetchall()\n",
    "\n",
    "    target_data = pd.DataFrame(rows,columns=['job_id','description'])\n",
    "\n",
    "    # Tag target data\n",
    "    target_data['tag'] = 1\n",
    "\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineData(target_data,non_target_data):\n",
    "    full_data = pd.concat([target_data,non_target_data],axis=0)\n",
    "\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_metrics(y_test,y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputIncorrectrResults(y_test,y_pred,full_data):\n",
    "    # Compare predictions with actual labels\n",
    "    incorrect_indices = [i for i in range(len(y_test)) if y_pred[i] != y_test.iloc[i]]\n",
    "\n",
    "    # Output incorrect predictions with job IDs\n",
    "    incorrect_predictions = full_data.iloc[y_test.index[incorrect_indices]]\n",
    "    incorrect_predictions['predicted_tag'] = y_pred[incorrect_indices]  # Add predicted tags to the dataframe\n",
    "\n",
    "    # Print or use incorrect predictions with job IDs\n",
    "    print(\"Incorrect Predictions with Job IDs:\")\n",
    "    for _, row in incorrect_predictions.iterrows():\n",
    "        print(f\"Job ID: {row['job_id']}\")\n",
    "        print(f\"Description: {row['description']}\")\n",
    "        print(f\"Actual Tag: {row['tag']}, Predicted Tag: {row['predicted_tag']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model,target_category):\n",
    "    temp = '../models/{}_classification_model_v2.pkl'\n",
    "    fileName = temp.format(target_category)\n",
    "    joblib.dump(model,fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCountVec(vec,target_category):\n",
    "    temp = '../models/{}_classification_vec_v2.pkl'\n",
    "    fileName = temp.format(target_category)\n",
    "    joblib.dump(vec,fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDistribution(y_train,y_test):\n",
    "    # Calculate distribution of classes (or any relevant feature)\n",
    "    train_distribution = y_train.value_counts(normalize=True) * 100\n",
    "    test_distribution = y_test.value_counts(normalize=True) * 100\n",
    "\n",
    "    # Print distributions\n",
    "    print(\"Training Set Distribution (%):\")\n",
    "    print(train_distribution)\n",
    "    print(\"\\nTesting Set Distribution (%):\")\n",
    "    print(test_distribution)\n",
    "\n",
    "    # Calculate exact counts of classes (or any relevant feature)\n",
    "    train_counts = y_train.value_counts()\n",
    "    test_counts = y_test.value_counts()\n",
    "\n",
    "    # Print counts\n",
    "    print(\"Training Set Counts:\")\n",
    "    print(train_counts)\n",
    "    print(\"\\nTesting Set Counts:\")\n",
    "    print(test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Category\n",
    "target_category = 'Laboratory and Research'\n",
    "\n",
    "target_data = getTargetData(target_category)\n",
    "non_target_data = getNonTargetData(target_category)\n",
    "\n",
    "full_data = combineData(target_data,non_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9552238805970149\n",
      "Recall: 0.7686622320768661\n",
      "F1 Score: 0.8213333333333332\n",
      "[[122   1]\n",
      " [  5   6]]\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    full_data['description'], full_data['tag'], test_size=0.2, random_state=42,stratify=full_data['tag'])\n",
    "\n",
    "# Transform text data to Count features\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression model count vectorizer\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_count)\n",
    "\n",
    "validation_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust threshold\n",
    "threshold = 0.3  # You can change this value\n",
    "y_pred = (y_pred_prob[:,1] >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9701492537313433\n",
      "Recall: 0.8595713229859572\n",
      "F1 Score: 0.8919354838709678\n",
      "[[122   1]\n",
      " [  3   8]]\n"
     ]
    }
   ],
   "source": [
    "validation_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "#saveModel(model,target_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Vec\n",
    "#saveCountVec(count_vectorizer,target_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
